# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Цель

На основе данных профилировщиков, оптимизировать исходный код приложения вплоть до 30 секунд выполняния в режиме *WALL_TIME* без отключения сборщика мусора.

## Задачи

1. Построить эффективный `feedback-loop` для быстрой оценки эффективности сделанных изменений.
2. Профилировать приложение всеми возможными инструментами, которые мы прошли за первую неделю.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *временная*

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: *как вы построили feedback_loop*

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- rbspy и ruby-prof показали, что именно Array#select является главной точкой роста, которую в последующем я и устранил используя банальных хэш.

### Ваша находка №2
- опробовав все профайлеры, решил на данном отчете остановиться на ruby-prof#Graph, он показал, что следующая точка роста была Array#all? - 37.26%, который как временное решение я решил крайне просто, методом uniq по всем браузерам с count. Но уже понимаю, что с кодом все крайне печально и очень манит батчем многое переписать, но приходится делать это степ бай степ

### Ваша находка 3
- следующая остановка была на `Array#each` 23.70% и многократном вызове `collect_stats_from_users`. Также смутил block.call, так как на сколько я помню yield куда быстрее

### Ваша находка 4
- следующий рефакторинг, решил сделать с `Array#map` 15.39%	опять же, который учавствует в формировании конечного результата в методе `collect_stats_from_users`. Решил вынести подсчет некоторых вещей на несколько шагов выше, а именно в парсинг `CSV` файла, попутно заменю `File.read(file_name).split("\n")` с последующим `Array.each` по нему, на `CSV#foreach`, И заметил непонятные лишние `split`-ы, тоже решил их прибрать.

### Ваша находка 5
- рефакторинг на 4 шаге оказался достаточно большим и не заметил момент, что в формировании json-а используется зачем-то `<Class::Date>#parse`, который после последнего рефакторинга занимает 7.51%. Убираю, так как он просто там не нужен, как и последующий `Array#map` для `iso8601`

### Ваша находка  6
-  решил затестить сколько займет времени парсинг всего файла и миллиона строк. Весь файл занял `43.46`, а миллион строк `12.12`. В принципе, осталось несколько очевидных для меня решений, которые можно улучшить. ruby-prof, показывает, значительное часть времени занимает метод `Array#count` 4.60%, их можно также убрать вынеся большую часть логики в парсинг CSV. Первым делом вынес подсчет количества юзеров. Дальше решил разобраться с количеством уникальных браузеров и количеством сессий. Попутно запуская между каждым изменением профайлер.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

